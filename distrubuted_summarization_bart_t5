{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Distributed Training Demo\n",
    "### Distributed Summarization with `transformers` scripts + `Trainer` and `samsum` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Tutorial](#Tutorial)  \n",
    "2. [Set up a development environment and install sagemaker](#Set-up-a-development-environment-and-install-sagemaker)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions) \n",
    "4. [Choose ðŸ¤— Transformers `examples/` script](#Choose-%F0%9F%A4%97-Transformers-examples/-script)  \n",
    "1. [Configure distributed training and hyperparameters](#Configure-distributed-training-and-hyperparameters)  \n",
    "2. [Create a `HuggingFace` estimator and start training](#Create-a-HuggingFace-estimator-and-start-training)   \n",
    "3. [Upload the fine-tuned model to huggingface.co](#Upload-the-fine-tuned-model-to-huggingface.co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "We will use the new [Hugging Face DLCs](https://github.com/aws/deep-learning-containers/tree/master/huggingface) and [Amazon SageMaker extension](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#huggingface-estimator) to train a distributed Seq2Seq-transformer model on `summarization` using the `transformers` and `datasets` libraries and upload it afterwards to [huggingface.co](http://huggingface.co) and test it.\n",
    "\n",
    "As [distributed training strategy](https://huggingface.co/transformers/sagemaker.html#distributed-training-data-parallel) we are going to use [SageMaker Data Parallelism](https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/), which has been built into the [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) API. To use data-parallelism we only have to define the `distribution` parameter in our `HuggingFace` estimator.\n",
    "\n",
    "```python\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "```\n",
    "\n",
    "In this tutorial, we will use an Amazon SageMaker Notebook Instance for running our training job. You can learnÂ [here how to set up a Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html).\n",
    "\n",
    "**What are we going to do:**\n",
    "\n",
    "- Set up a development environment and install sagemaker\n",
    "- Chose ðŸ¤— Transformers `examples/` script\n",
    "- Configure distributed training and hyperparameters\n",
    "- Create a `HuggingFace` estimator and start training\n",
    "- Upload the fine-tuned model to [huggingface.co](http://huggingface.co)\n",
    "- Test inference\n",
    "\n",
    "### Model and Dataset\n",
    "\n",
    "We are going to fine-tune [facebook/bart-base](https://huggingface.co/facebook/bart-base) on the [samsum](https://huggingface.co/datasets/samsum) dataset. *\"BART is sequence-to-sequence model trained with denoising as pretraining objective.\"* [[REF](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.md)]\n",
    "\n",
    "The `samsum` dataset contains about 16k messenger-like conversations with summaries. \n",
    "\n",
    "```python\n",
    "{'id': '13818513',\n",
    " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.',\n",
    " 'dialogue': \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"}\n",
    "```\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up a development environment and install sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_**Note:**Â The use of Jupyter is optional: We could also launch SageMaker Training jobs from anywhere we have an SDK installed, connectivity to the cloud and appropriate permissions, such as a Laptop, another IDE or a task scheduler like Airflow or AWS Step Functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.48.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (2.67.0)\n",
      "Requirement already satisfied: transformers==4.6.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (4.6.1)\n",
      "Requirement already satisfied: datasets[s3]==1.6.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (1.6.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (4.8.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (1.19.5)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (21.0)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.0.46)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2.26.0)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (4.49.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (3.3.2)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2020.11.13)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2.0.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (1.1.5)\n",
      "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (5.0.0)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.3.4)\n",
      "Requirement already satisfied: botocore==1.19.52 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (1.19.52)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Requirement already satisfied: boto3==1.16.43 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (1.16.43)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.6.2) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.6.2) (0.3.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (2.8.2)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.8)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (3.19.0)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (21.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging->transformers==4.6.1) (3.0.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker>=2.48.0) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2.0.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->datasets[s3]==1.6.2) (2021.3)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (0.3.0)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from s3fs->datasets[s3]==1.6.2) (1.2.2)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (0.7.1)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.12.1)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.7.4.post0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.0.1)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (5.1.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.6.3)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (1.0.1)\n",
      "\u001B[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade\n",
    "#!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n",
    "#!sudo yum install git-lfs -y\n",
    "#!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**upgrade ipywidgets for `datasets` library and restart kernel, only needed when prerpocessing is done in the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "#!conda install -c conda-forge ipywidgets -y\n",
    "#IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAM role arn used for running training: arn:aws:iam::847380964353:role/service-role/AmazonSageMaker-ExecutionRole-20200409T140000\n",
      "S3 bucket used for storing artifacts: sagemaker-us-east-2-847380964353\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"IAM role arn used for running training: {role}\")\n",
    "print(f\"S3 bucket used for storing artifacts: {sess.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose ðŸ¤— Transformers `examples/` script\n",
    "\n",
    "The [ðŸ¤— Transformers repository](https://github.com/huggingface/transformers/tree/master/examples) contains several `examples/`scripts for fine-tuning models on tasks from `language-modeling` to `token-classification`. In our case, we are using the `run_summarization.py` from the `seq2seq/` examples. \n",
    "\n",
    "_**Note**: you can use this tutorial identical to train your model on a different examples script._\n",
    "\n",
    "Since theÂ `HuggingFace`Â Estimator has git support built-in, we can specify aÂ [training script that is stored in a GitHub repository](https://sagemaker.readthedocs.io/en/stable/overview.html#use-scripts-stored-in-a-git-repository)Â asÂ `entry_point`Â andÂ `source_dir`.\n",
    "\n",
    "We are going to use the `transformers 4.4.2` DLC which means we need to configure the `v4.4.2` as the branch to pull the compatible example scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'} # v4.6.1 is referring to the `transformers_version` you use in the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure distributed training and hyperparameters\n",
    "\n",
    "Next, we will define our `hyperparameters` and configure our distributed training strategy. As hyperparameter, we can define any [Seq2SeqTrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainingarguments) and the ones defined in [run_summarization.py](https://github.com/huggingface/transformers/tree/master/examples/seq2seq#sequence-to-sequence-training-and-evaluation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'per_device_train_batch_size': 1,\n",
    "                 'per_device_eval_batch_size': 1,\n",
    "                 'model_name_or_path': 'facebook/bart-large-cnn',\n",
    "                 'dataset_name': 'samsum',\n",
    "                 'do_train': True,\n",
    "                 'do_eval': True,\n",
    "                 'do_predict': True,\n",
    "                 'predict_with_generate': True,\n",
    "                 'output_dir': '/opt/ml/model',\n",
    "                 'num_train_epochs': 1,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'seed': 7,\n",
    "                 'save_total_limit':3,\n",
    "                 'fp16': False,\n",
    "                 }\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "#distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a `HuggingFace` estimator and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "      entry_point='run_summarization.py', # script\n",
    "      source_dir='./examples/pytorch/summarization', # relative path to example\n",
    "      git_config=git_config,\n",
    "      instance_type='ml.p3.16xlarge',\n",
    "      instance_count=1,\n",
    "      transformers_version='4.6',\n",
    "      pytorch_version='1.7',\n",
    "      py_version='py36',\n",
    "      volume_size=500, #instance memory\n",
    "      role=role,\n",
    "      base_job_name='bart-en',\n",
    "      hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 08:13:59 Starting - Starting the training job...\n",
      "2021-11-02 08:14:22 Starting - Launching requested ML instancesProfilerReport-1635840833: InProgress\n",
      "......\n",
      "2021-11-02 08:15:23 Starting - Preparing the instances for training.........\n",
      "2021-11-02 08:16:51 Downloading - Downloading input data\n",
      "2021-11-02 08:16:51 Training - Downloading the training image..................\n",
      "2021-11-02 08:19:58 Training - Training image download completed. Training in progress..\u001B[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001B[0m\n",
      "\u001B[34mbash: no job control in this shell\u001B[0m\n",
      "\u001B[34m2021-11-02 08:19:58,775 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001B[0m\n",
      "\u001B[34m2021-11-02 08:19:58,799 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001B[0m\n",
      "\u001B[34m2021-11-02 08:19:58,806 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001B[0m\n",
      "\u001B[34m2021-11-02 08:19:59,127 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001B[0m\n",
      "\u001B[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.1.91)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (3.17.1)\u001B[0m\n",
      "\u001B[34mCollecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\u001B[0m\n",
      "\u001B[34mCollecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\u001B[0m\n",
      "\u001B[34mCollecting py7zr\n",
      "  Downloading py7zr-0.16.2-py3-none-any.whl (66 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (1.7.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.0.8)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.3)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.25.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.1.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2021.5.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.49.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.19.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.11.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.8)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (20.9)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.10.0.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.12)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.4)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.12.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.25.11)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 3)) (1.16.0)\u001B[0m\n",
      "\u001B[34mCollecting absl-py\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 5)) (1.0.1)\u001B[0m\n",
      "\u001B[34mCollecting regex>=2021.8.3\n",
      "  Downloading regex-2021.11.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 5)) (7.1.2)\u001B[0m\n",
      "\u001B[34mCollecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\u001B[0m\n",
      "\u001B[34mCollecting pyppmd>=0.17.0\n",
      "  Downloading pyppmd-0.17.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\u001B[0m\n",
      "\u001B[34mCollecting brotli>=1.0.9\n",
      "  Downloading Brotli-1.0.9-cp36-cp36m-manylinux1_x86_64.whl (357 kB)\u001B[0m\n",
      "\u001B[34mCollecting pybcj>=0.5.0\n",
      "  Downloading pybcj-0.5.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47 kB)\u001B[0m\n",
      "\u001B[34mCollecting texttable\n",
      "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\u001B[0m\n",
      "\u001B[34mCollecting pyzstd>=0.14.4\n",
      "  Downloading pyzstd-0.15.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\u001B[0m\n",
      "\u001B[34mCollecting pycryptodomex>=3.6.6\n",
      "  Downloading pycryptodomex-3.11.0-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 1)) (3.4.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 1)) (2.4.7)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2021.1)\u001B[0m\n",
      "\u001B[34mInstalling collected packages: regex, texttable, pyzstd, pyppmd, pycryptodomex, pybcj, nltk, multivolumefile, brotli, absl-py, rouge-score, py7zr\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\u001B[0m\n",
      "\u001B[34mSuccessfully installed absl-py-0.15.0 brotli-1.0.9 multivolumefile-0.2.3 nltk-3.6.5 py7zr-0.16.2 pybcj-0.5.0 pycryptodomex-3.11.0 pyppmd-0.17.1 pyzstd-0.15.0 regex-2021.11.1 rouge-score-0.0.4 texttable-1.6.4\u001B[0m\n",
      "\u001B[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001B[0m\n",
      "\u001B[34m2021-11-02 08:20:06,456 sagemaker-training-toolkit INFO     Invoking user script\u001B[0m\n",
      "\u001B[34mTraining Env:\u001B[0m\n",
      "\u001B[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"per_device_eval_batch_size\": 1,\n",
      "        \"predict_with_generate\": true,\n",
      "        \"seed\": 7,\n",
      "        \"do_predict\": true,\n",
      "        \"do_train\": true,\n",
      "        \"dataset_name\": \"samsum\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"do_eval\": true,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"model_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "        \"fp16\": false\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-11-02-08-13-53-557\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-847380964353/huggingface-pytorch-training-2021-11-02-08-13-53-557/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_summarization\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_summarization.py\"\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34mEnvironment variables:\u001B[0m\n",
      "\u001B[34mSM_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_NETWORK_INTERFACE_NAME=eth0\u001B[0m\n",
      "\u001B[34mSM_HPS={\"dataset_name\":\"samsum\",\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"fp16\":false,\"learning_rate\":5e-05,\"model_name_or_path\":\"facebook/bart-large-cnn\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":1,\"per_device_train_batch_size\":1,\"predict_with_generate\":true,\"seed\":7}\u001B[0m\n",
      "\u001B[34mSM_USER_ENTRY_POINT=run_summarization.py\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_PARAMS={}\u001B[0m\n",
      "\u001B[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001B[0m\n",
      "\u001B[34mSM_INPUT_DATA_CONFIG={}\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001B[0m\n",
      "\u001B[34mSM_CHANNELS=[]\u001B[0m\n",
      "\u001B[34mSM_CURRENT_HOST=algo-1\u001B[0m\n",
      "\u001B[34mSM_MODULE_NAME=run_summarization\u001B[0m\n",
      "\u001B[34mSM_LOG_LEVEL=20\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001B[0m\n",
      "\u001B[34mSM_INPUT_DIR=/opt/ml/input\u001B[0m\n",
      "\u001B[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DIR=/opt/ml/output\u001B[0m\n",
      "\u001B[34mSM_NUM_CPUS=8\u001B[0m\n",
      "\u001B[34mSM_NUM_GPUS=1\u001B[0m\n",
      "\u001B[34mSM_MODEL_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-847380964353/huggingface-pytorch-training-2021-11-02-08-13-53-557/source/sourcedir.tar.gz\u001B[0m\n",
      "\u001B[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_name\":\"samsum\",\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"fp16\":false,\"learning_rate\":5e-05,\"model_name_or_path\":\"facebook/bart-large-cnn\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":1,\"per_device_train_batch_size\":1,\"predict_with_generate\":true,\"seed\":7},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-11-02-08-13-53-557\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-847380964353/huggingface-pytorch-training-2021-11-02-08-13-53-557/source/sourcedir.tar.gz\",\"module_name\":\"run_summarization\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_summarization.py\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ARGS=[\"--dataset_name\",\"samsum\",\"--do_eval\",\"True\",\"--do_predict\",\"True\",\"--do_train\",\"True\",\"--fp16\",\"False\",\"--learning_rate\",\"5e-05\",\"--model_name_or_path\",\"facebook/bart-large-cnn\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"1\",\"--predict_with_generate\",\"True\",\"--seed\",\"7\"]\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001B[0m\n",
      "\u001B[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001B[0m\n",
      "\u001B[34mSM_HP_PREDICT_WITH_GENERATE=true\u001B[0m\n",
      "\u001B[34mSM_HP_SEED=7\u001B[0m\n",
      "\u001B[34mSM_HP_DO_PREDICT=true\u001B[0m\n",
      "\u001B[34mSM_HP_DO_TRAIN=true\u001B[0m\n",
      "\u001B[34mSM_HP_DATASET_NAME=samsum\u001B[0m\n",
      "\u001B[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001B[0m\n",
      "\u001B[34mSM_HP_DO_EVAL=true\u001B[0m\n",
      "\u001B[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001B[0m\n",
      "\u001B[34mSM_HP_LEARNING_RATE=5e-05\u001B[0m\n",
      "\u001B[34mSM_HP_MODEL_NAME_OR_PATH=facebook/bart-large-cnn\u001B[0m\n",
      "\u001B[34mSM_HP_FP16=false\u001B[0m\n",
      "\u001B[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001B[0m\n",
      "\u001B[34mInvoking script with the following command:\u001B[0m\n",
      "\u001B[34m/opt/conda/bin/python3.6 run_summarization.py --dataset_name samsum --do_eval True --do_predict True --do_train True --fp16 False --learning_rate 5e-05 --model_name_or_path facebook/bart-large-cnn --num_train_epochs 1 --output_dir /opt/ml/model --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --predict_with_generate True --seed 7\u001B[0m\n",
      "\u001B[34m11/02/2021 08:20:11 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001B[0m\n",
      "\u001B[34m11/02/2021 08:20:11 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/opt/ml/model', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Nov02_08-20-11_algo-1', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=7, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/opt/ml/model', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\u001B[0m\n",
      "\u001B[34mDownloading and preparing dataset samsum/samsum (download: 2.81 MiB, generated: 10.04 MiB, post-processed: Unknown size, total: 12.85 MiB) to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6...\u001B[0m\n",
      "\u001B[34mDataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6. Subsequent calls will reuse this data.\u001B[0m\n",
      "\u001B[34mhttps://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgd3rrmyo\u001B[0m\n",
      "\u001B[34mstoring https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\u001B[0m\n",
      "\u001B[34mcreating metadata file for /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\u001B[0m\n",
      "\u001B[34mloading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\u001B[0m\n",
      "\u001B[34mModel config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34mloading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\u001B[0m\n",
      "\u001B[34mModel config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34mhttps://huggingface.co/facebook/bart-large-cnn/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpoizy35cj\u001B[0m\n",
      "\u001B[34mstoring https://huggingface.co/facebook/bart-large-cnn/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/4d8eeedc3498bc73a4b72411ebb3219209b305663632d77a6f16e60790b18038.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001B[0m\n",
      "\u001B[34mcreating metadata file for /root/.cache/huggingface/transformers/4d8eeedc3498bc73a4b72411ebb3219209b305663632d77a6f16e60790b18038.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001B[0m\n",
      "\u001B[34mhttps://huggingface.co/facebook/bart-large-cnn/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9g5h9r16\u001B[0m\n",
      "\u001B[34mstoring https://huggingface.co/facebook/bart-large-cnn/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/0ddddd3ca9e107b17a6901c92543692272af1c3238a8d7549fa937ba0057bbcf.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001B[0m\n",
      "\u001B[34mcreating metadata file for /root/.cache/huggingface/transformers/0ddddd3ca9e107b17a6901c92543692272af1c3238a8d7549fa937ba0057bbcf.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001B[0m\n",
      "\u001B[34mhttps://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp08g10z7_\u001B[0m\n",
      "\u001B[34mstoring https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/55c96bd962ce1d360fde4947619318f1b4eb551430de678044699cbfeb99de6a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001B[0m\n",
      "\u001B[34mcreating metadata file for /root/.cache/huggingface/transformers/55c96bd962ce1d360fde4947619318f1b4eb551430de678044699cbfeb99de6a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001B[0m\n",
      "\u001B[34mloading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/4d8eeedc3498bc73a4b72411ebb3219209b305663632d77a6f16e60790b18038.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001B[0m\n",
      "\u001B[34mloading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/0ddddd3ca9e107b17a6901c92543692272af1c3238a8d7549fa937ba0057bbcf.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001B[0m\n",
      "\u001B[34mloading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/55c96bd962ce1d360fde4947619318f1b4eb551430de678044699cbfeb99de6a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001B[0m\n",
      "\u001B[34mloading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/added_tokens.json from cache at None\u001B[0m\n",
      "\u001B[34mloading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/special_tokens_map.json from cache at None\u001B[0m\n",
      "\u001B[34mloading file https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer_config.json from cache at None\u001B[0m\n",
      "\u001B[34mhttps://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2ktjscfv\u001B[0m\n",
      "\u001B[34mstoring https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\u001B[0m\n",
      "\u001B[34mcreating metadata file for /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\u001B[0m\n",
      "\u001B[34mloading weights file https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\u001B[0m\n",
      "\u001B[34mAll model checkpoint weights were used when initializing BartForConditionalGeneration.\u001B[0m\n",
      "\u001B[34mAll the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\u001B[0m\n",
      "\u001B[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\u001B[0m\n",
      "\u001B[34m***** Running training *****\n",
      "  Num examples = 14732\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14732\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.084 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.230 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.231 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.231 algo-1:32 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.233 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.233 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.396 algo-1:32 INFO hook.py:591] name:model.shared.weight count_params:51471360\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.397 algo-1:32 INFO hook.py:591] name:model.encoder.embed_positions.weight count_params:1050624\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.397 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.397 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.397 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.397 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.397 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.398 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.398 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.398 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.398 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.398 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.398 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.398 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.398 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.399 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.399 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.399 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.399 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.399 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.399 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.399 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.399 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.399 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.400 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.400 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.400 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.400 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.400 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.400 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.400 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.400 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.400 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.401 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.401 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.401 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.401 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.401 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.401 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.401 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.401 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.401 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.401 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.402 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.402 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.402 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.402 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.402 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.402 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.402 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.402 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.402 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.403 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.403 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.403 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.403 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.403 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.403 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.403 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.403 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.403 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.404 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.404 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.404 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.404 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.404 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.404 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.404 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.405 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.405 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.405 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.405 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.405 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.405 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.405 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.405 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.405 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.405 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.406 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.406 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.406 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.406 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.406 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.406 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.406 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.406 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.407 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.407 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.407 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.407 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.407 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.408 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.408 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.408 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.408 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.408 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.408 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.408 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.408 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.409 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.409 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.409 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.409 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.409 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.409 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.409 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.410 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.410 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.410 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.410 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.410 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.410 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.411 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.411 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.411 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.411 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.411 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.411 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.412 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.412 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.412 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.412 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.412 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.412 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.412 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.412 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.412 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.413 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.413 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.413 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.413 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.413 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.413 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.413 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.414 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.414 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.414 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.414 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.414 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.414 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.414 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.414 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.415 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.415 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.415 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.415 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.415 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.415 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.416 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.416 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.416 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.416 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.416 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.416 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.416 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.416 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.417 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.417 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.417 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.417 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.417 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.417 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.417 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.417 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.418 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.418 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.418 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.418 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.418 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.418 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.418 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.418 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.418 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.419 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.419 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.419 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.419 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.419 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.419 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.419 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.419 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.419 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.420 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.420 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.420 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.420 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.420 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.420 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.420 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.420 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.421 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.421 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.421 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.421 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.421 algo-1:32 INFO hook.py:591] name:model.encoder.layernorm_embedding.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.421 algo-1:32 INFO hook.py:591] name:model.encoder.layernorm_embedding.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.421 algo-1:32 INFO hook.py:591] name:model.decoder.embed_positions.weight count_params:1050624\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.421 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.422 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.422 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.422 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.422 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.422 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.423 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.423 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.423 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.423 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.423 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.423 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.423 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.423 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.424 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.424 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.424 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.424 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.424 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.424 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.424 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.424 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.424 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.425 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.425 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.425 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.425 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.425 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.425 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.425 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.425 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.425 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.425 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.426 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.426 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.426 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.426 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.426 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.426 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.427 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.427 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.427 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.427 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.427 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.427 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.427 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.427 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.428 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.428 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.428 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.428 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.428 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.428 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.428 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.428 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.429 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.429 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.429 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.429 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.429 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.429 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.429 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.429 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.429 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.429 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.430 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.430 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.430 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.430 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.430 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.430 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.430 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.430 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.430 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.431 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.431 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.431 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.431 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.431 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.431 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.431 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.432 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.432 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.432 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.432 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.432 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.433 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.433 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.433 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.433 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.433 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.433 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.433 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.434 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.434 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.434 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.434 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.434 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.434 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.434 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.434 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.435 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.435 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.435 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.435 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.435 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.435 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.435 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.435 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.436 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.436 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.436 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.436 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.436 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.436 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.436 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.437 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.437 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.437 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.437 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.437 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.437 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.438 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.438 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.438 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.438 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.438 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.438 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.438 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.438 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.439 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.439 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.439 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.439 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.439 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.439 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.439 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.439 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.440 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.440 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.440 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.440 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.440 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.440 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.440 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.440 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.440 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.441 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.441 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.441 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.441 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.441 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.441 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.441 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.442 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.442 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.442 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.442 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.442 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.442 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.442 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.442 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.443 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.443 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.443 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.443 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.443 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.444 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.444 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.444 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.444 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.444 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.444 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.444 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.444 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.445 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.445 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.445 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.445 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.445 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.445 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.445 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.445 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.446 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.446 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.446 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.446 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.446 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.446 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.446 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.446 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.446 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.447 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.447 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.447 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.447 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.447 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.447 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.447 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.447 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.448 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.448 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.448 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.448 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.448 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.448 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.448 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.448 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.449 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.449 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.449 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.449 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.449 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.449 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.450 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.450 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.450 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.450 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.450 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.450 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.450 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.450 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.450 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.451 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.451 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.451 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.451 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.451 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.451 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.451 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.451 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.452 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.452 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.452 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.452 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.452 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.452 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.452 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.452 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.453 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.453 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.453 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.453 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.453 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.453 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.453 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.453 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.453 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.454 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.454 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.454 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.454 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.454 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.454 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.454 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.454 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.454 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.454 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.455 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.455 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.455 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.455 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.455 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.455 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.455 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.456 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.456 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.456 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.456 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.456 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.456 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.456 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.456 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.457 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.457 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.457 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.457 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.457 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.457 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.457 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.457 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.457 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.458 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.458 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.458 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.458 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.458 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.458 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.459 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.459 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.459 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.459 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.459 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.459 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.459 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.460 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.460 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.k_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.460 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.k_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.460 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.v_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.460 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.v_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.460 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.q_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.460 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.q_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.460 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.out_proj.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.461 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.out_proj.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.461 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.461 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.461 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc1.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.461 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc1.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.461 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc2.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.461 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc2.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.461 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.final_layer_norm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.461 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.final_layer_norm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.462 algo-1:32 INFO hook.py:591] name:model.decoder.layernorm_embedding.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.462 algo-1:32 INFO hook.py:591] name:model.decoder.layernorm_embedding.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.462 algo-1:32 INFO hook.py:593] Total Trainable Params: 406291456\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.462 algo-1:32 INFO hook.py:425] Monitoring the collections: losses\u001B[0m\n",
      "\u001B[34m[2021-11-02 08:21:12.464 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001B[0m\n",
      "\u001B[34m{'loss': 2.0084, 'learning_rate': 4.830301384740701e-05, 'epoch': 0.03}\u001B[0m\n",
      "\u001B[34mSaving model checkpoint to /opt/ml/model/checkpoint-500\u001B[0m\n",
      "\u001B[34mConfiguration saved in /opt/ml/model/checkpoint-500/config.json\u001B[0m\n",
      "\u001B[34mModel weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001B[0m\n",
      "\u001B[34mtokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001B[0m\n",
      "\u001B[34mSpecial tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m{'loss': 1.9852, 'learning_rate': 4.660602769481401e-05, 'epoch': 0.07}\u001B[0m\n",
      "\u001B[34mSaving model checkpoint to /opt/ml/model/checkpoint-1000\u001B[0m\n",
      "\u001B[34mConfiguration saved in /opt/ml/model/checkpoint-1000/config.json\u001B[0m\n",
      "\u001B[34mModel weights saved in /opt/ml/model/checkpoint-1000/pytorch_model.bin\u001B[0m\n",
      "\u001B[34mtokenizer config file saved in /opt/ml/model/checkpoint-1000/tokenizer_config.json\u001B[0m\n",
      "\u001B[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1000/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m{'loss': 1.9393, 'learning_rate': 4.490904154222102e-05, 'epoch': 0.1}\u001B[0m\n",
      "\u001B[34mSaving model checkpoint to /opt/ml/model/checkpoint-1500\u001B[0m\n",
      "\u001B[34mConfiguration saved in /opt/ml/model/checkpoint-1500/config.json\u001B[0m\n",
      "\u001B[34mModel weights saved in /opt/ml/model/checkpoint-1500/pytorch_model.bin\u001B[0m\n",
      "\u001B[34mtokenizer config file saved in /opt/ml/model/checkpoint-1500/tokenizer_config.json\u001B[0m\n",
      "\u001B[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1500/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m{'loss': 1.8081, 'learning_rate': 4.321205538962802e-05, 'epoch': 0.14}\u001B[0m\n",
      "\u001B[34mSaving model checkpoint to /opt/ml/model/checkpoint-2000\u001B[0m\n",
      "\u001B[34mConfiguration saved in /opt/ml/model/checkpoint-2000/config.json\u001B[0m\n",
      "\u001B[34mModel weights saved in /opt/ml/model/checkpoint-2000/pytorch_model.bin\u001B[0m\n",
      "\u001B[34mtokenizer config file saved in /opt/ml/model/checkpoint-2000/tokenizer_config.json\u001B[0m\n",
      "\u001B[34mSpecial tokens file saved in /opt/ml/model/checkpoint-2000/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m{'loss': 1.8947, 'learning_rate': 4.151506923703503e-05, 'epoch': 0.17}\u001B[0m\n",
      "\u001B[34mSaving model checkpoint to /opt/ml/model/checkpoint-2500\u001B[0m\n",
      "\u001B[34mConfiguration saved in /opt/ml/model/checkpoint-2500/config.json\u001B[0m\n",
      "\u001B[34mModel weights saved in /opt/ml/model/checkpoint-2500/pytorch_model.bin\u001B[0m\n",
      "\u001B[34mtokenizer config file saved in /opt/ml/model/checkpoint-2500/tokenizer_config.json\u001B[0m\n",
      "\u001B[34mSpecial tokens file saved in /opt/ml/model/checkpoint-2500/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m{'loss': 1.8156, 'learning_rate': 3.9818083084442033e-05, 'epoch': 0.2}\u001B[0m\n",
      "\u001B[34mSaving model checkpoint to /opt/ml/model/checkpoint-3000\u001B[0m\n",
      "\u001B[34mConfiguration saved in /opt/ml/model/checkpoint-3000/config.json\u001B[0m\n",
      "\u001B[34mModel weights saved in /opt/ml/model/checkpoint-3000/pytorch_model.bin\u001B[0m\n",
      "\u001B[34mtokenizer config file saved in /opt/ml/model/checkpoint-3000/tokenizer_config.json\u001B[0m\n",
      "\u001B[34mSpecial tokens file saved in /opt/ml/model/checkpoint-3000/special_tokens_map.json\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# starting the train job\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = '''Jeff: Can I train a ðŸ¤— Transformers model on Amazon SageMaker? \n",
    "    Philipp: Sure you can use the new Hugging Face Deep Learning Container. \n",
    "    Jeff: ok.\n",
    "    Jeff: and how can I get started? \n",
    "    Jeff: where can I find documentation? \n",
    "    Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face                                           \n",
    "    '''\n",
    "\n",
    "data= {\"inputs\":conversation}\n",
    "\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the fine-tuned model to [huggingface.co](http://huggingface.co)\n",
    "\n",
    "We can download our model from Amazon S3 and unzip it using the following snippet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "local_path = 'my_bart_model'\n",
    "\n",
    "os.makedirs(local_path, exist_ok = True)\n",
    "\n",
    "# download model from S3\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path=local_path, # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess # sagemaker session used for training the model\n",
    ")\n",
    "\n",
    "# unzip model\n",
    "tar = tarfile.open(f\"{local_path}/model.tar.gz\", \"r:gz\")\n",
    "tar.extractall(path=local_path)\n",
    "tar.close()\n",
    "os.remove(f\"{local_path}/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are going to upload our model to [huggingface.co](http://huggingface.co) we need to create a `model_card`. The `model_card` describes the model includes hyperparameters, results and which dataset was used for training. To create a `model_card` we create a `README.md` in our `local_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read eval and test results \n",
    "with open(f\"{local_path}/eval_results.json\") as f:\n",
    "    eval_results_raw = json.load(f)\n",
    "    eval_results={}\n",
    "    eval_results[\"eval_rouge1\"] = eval_results_raw[\"eval_rouge1\"]\n",
    "    eval_results[\"eval_rouge2\"] = eval_results_raw[\"eval_rouge2\"]\n",
    "    eval_results[\"eval_rougeL\"] = eval_results_raw[\"eval_rougeL\"]\n",
    "    eval_results[\"eval_rougeLsum\"] = eval_results_raw[\"eval_rougeLsum\"]\n",
    "\n",
    "with open(f\"{local_path}/test_results.json\") as f:\n",
    "    test_results_raw = json.load(f)\n",
    "    test_results={}\n",
    "    test_results[\"test_rouge1\"] = test_results_raw[\"test_rouge1\"]\n",
    "    test_results[\"test_rouge2\"] = test_results_raw[\"test_rouge2\"]\n",
    "    test_results[\"test_rougeL\"] = test_results_raw[\"test_rougeL\"]\n",
    "    test_results[\"test_rougeLsum\"] = test_results_raw[\"test_rougeLsum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After we extract all the metrics we want to include we are going to create our `README.md`. Additionally to the automated generation of the results table we add the metrics manually to the `metadata` of our model card under `model-index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_results)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "MODEL_CARD_TEMPLATE = \"\"\"\n",
    "---\n",
    "language: en\n",
    "tags:\n",
    "- sagemaker\n",
    "- bart\n",
    "- summarization\n",
    "license: apache-2.0\n",
    "datasets:\n",
    "- samsum\n",
    "model-index:\n",
    "- name: {model_name}\n",
    "  results:\n",
    "  - task: \n",
    "      name: Abstractive Text Summarization\n",
    "      type: abstractive-text-summarization\n",
    "    dataset:\n",
    "      name: \"SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\" \n",
    "      type: samsum\n",
    "    metrics:\n",
    "       - name: Validation ROGUE-1\n",
    "         type: rogue-1\n",
    "         value: 42.621\n",
    "       - name: Validation ROGUE-2\n",
    "         type: rogue-2\n",
    "         value: 21.9825\n",
    "       - name: Validation ROGUE-L\n",
    "         type: rogue-l\n",
    "         value: 33.034\n",
    "       - name: Test ROGUE-1\n",
    "         type: rogue-1\n",
    "         value: 41.3174\n",
    "       - name: Test ROGUE-2\n",
    "         type: rogue-2\n",
    "         value: 20.8716\n",
    "       - name: Test ROGUE-L\n",
    "         type: rogue-l\n",
    "         value: 32.1337\n",
    "widget:\n",
    "- text: | \n",
    "    Jeff: Can I train a ðŸ¤— Transformers model on Amazon SageMaker? \n",
    "    Philipp: Sure you can use the new Hugging Face Deep Learning Container. \n",
    "    Jeff: ok.\n",
    "    Jeff: and how can I get started? \n",
    "    Jeff: where can I find documentation? \n",
    "    Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face \n",
    "---\n",
    "## `{model_name}`\n",
    "This model was trained using Amazon SageMaker and the new Hugging Face Deep Learning container.\n",
    "For more information look at:\n",
    "- [ðŸ¤— Transformers Documentation: Amazon SageMaker](https://huggingface.co/transformers/sagemaker.html)\n",
    "- [Example Notebooks](https://github.com/huggingface/notebooks/tree/master/sagemaker)\n",
    "- [Amazon SageMaker documentation for Hugging Face](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)\n",
    "- [Python SDK SageMaker documentation for Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html)\n",
    "- [Deep Learning Container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers)\n",
    "## Hyperparameters\n",
    "    {hyperparameters}\n",
    "## Usage\n",
    "    from transformers import pipeline\n",
    "    summarizer = pipeline(\"summarization\", model=\"philschmid/{model_name}\")\n",
    "    conversation = '''Jeff: Can I train a ðŸ¤— Transformers model on Amazon SageMaker? \n",
    "    Philipp: Sure you can use the new Hugging Face Deep Learning Container. \n",
    "    Jeff: ok.\n",
    "    Jeff: and how can I get started? \n",
    "    Jeff: where can I find documentation? \n",
    "    Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face                                           \n",
    "    '''\n",
    "    nlp(conversation)\n",
    "## Results\n",
    "| key | value |\n",
    "| --- | ----- |\n",
    "{eval_table}\n",
    "{test_table}\n",
    "\"\"\"\n",
    "\n",
    "# Generate model card (todo: add more data from Trainer)\n",
    "model_card = MODEL_CARD_TEMPLATE.format(\n",
    "    model_name=f\"{hyperparameters['model_name_or_path'].split('/')[1]}-{hyperparameters['dataset_name']}\",\n",
    "    hyperparameters=json.dumps(hyperparameters, indent=4, sort_keys=True),\n",
    "    eval_table=\"\\n\".join(f\"| {k} | {v} |\" for k, v in eval_results.items()),\n",
    "    test_table=\"\\n\".join(f\"| {k} | {v} |\" for k, v in test_results.items()),\n",
    ")\n",
    "with open(f\"{local_path}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have our unzipped model and model card located in `my_bart_model` we can use the either `huggingface_hub` SDK to create a repository and upload it to [huggingface.co](http://huggingface.co) or go to https://huggingface.co/new an create a new repository and upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from huggingface_hub import HfApi, Repository\n",
    "\n",
    "hf_username = \"philschmid\" # your username on huggingface.co\n",
    "hf_email = \"philipp@huggingface.co\" # email used for commit\n",
    "repository_name = f\"{hyperparameters['model_name_or_path'].split('/')[1]}-{hyperparameters['dataset_name']}\" # repository name on huggingface.co\n",
    "password = getpass(\"Enter your password:\") # creates a prompt for entering password\n",
    "\n",
    "# get hf token\n",
    "token = HfApi().login(username=hf_username, password=password)\n",
    "\n",
    "# create repository\n",
    "repo_url = HfApi().create_repo(token=token, name=repository_name, exist_ok=True)\n",
    "\n",
    "# create a Repository instance\n",
    "model_repo = Repository(use_auth_token=token,\n",
    "                        clone_from=repo_url,\n",
    "                        local_dir=local_path,\n",
    "                        git_user=hf_username,\n",
    "                        git_email=hf_email)\n",
    "\n",
    "# push model to the hub\n",
    "model_repo.push_to_hub()\n",
    "\n",
    "print(f\"https://huggingface.co/{hf_username}/{repository_name}\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}